{"meta":{"title":"zuoyang's blog","subtitle":null,"description":"welcome to my blog","author":"zuoyang","url":"https://www.zuoyangblog.com"},"pages":[{"title":"","date":"2018-11-15T10:26:40.000Z","updated":"2018-11-15T10:26:40.000Z","comments":true,"path":"about/index.html","permalink":"https://www.zuoyangblog.com/about/index.html","excerpt":"","text":"hi,大家好！欢迎大家来到我的blog，希望以后能跟各位大佬一起学习进步。 我叫左杨，目前就职于蚂蚁金服人工智能部，从事于蚂蚁人工智能相关的开发工作，对Machine Learning，Deep Learning比较感兴趣。"},{"title":"Categories","date":"2018-11-15T13:20:00.000Z","updated":"2018-11-15T13:20:00.000Z","comments":true,"path":"categories/index.html","permalink":"https://www.zuoyangblog.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-11-15T04:33:46.000Z","updated":"2018-11-15T04:33:46.000Z","comments":true,"path":"tags/index.html","permalink":"https://www.zuoyangblog.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"手动部署ceph集群","slug":"manual-deploy-ceph","date":"2018-11-15T07:54:19.000Z","updated":"2018-11-15T12:15:51.000Z","comments":true,"path":"post/c3d7e91e.html","link":"","permalink":"https://www.zuoyangblog.com/post/c3d7e91e.html","excerpt":"","text":"1、机器选择1.1 系统要求ceph 最新 LTS 版本 (luminous) 推荐 linux 内核版本 4.1.4 及以上, 最低版本要求 3.10.*。 1.2 服务器这里选择三台服务器来部署ceph集群，一台Mon+五台OSD 节点 服务 cluster network public network 192.168.226.20 osd.1,mon.node2 192.168.226.0/24 192.168.226.0/24 192.168.226.21 osd.4 192.168.226.0/24 192.168.226.0/24 192.168.226.22 osd.2, mon.node1 192.168.226.0/24 192.168.226.0/24 192.168.226.96 osd.3,mon.node3 192.168.226.0/24 192.168.226.0/24 192.168.226.106 osd.0 192.168.226.0/24 192.168.226.0/24 每个节点只能使用1块磁盘部署osd。所以，集群共有5个osd进程，3个monitor进程。 cluster network 是处理osd间的数据复制，数据重平衡，osd进程心跳检测的网络，其不对外提供服务，只在各个osd节点间通信，本文使用eth1网卡作为cluster network，三个节点网卡eth1桥接到同一个网桥br1上 2、环境配置配置每个节点的host文件，在 /etc/hosts文件中添加如下内容： 123192.168.226.20 ceph-1192.168.226.22 ceph-2192.168.226.96 ceph-3 2.2 ceph节点安装你的管理节点必须能够通过 SSH 无密码地访问各 Ceph 节点。如果 ceph-deploy 以某个普通用户登录，那么这个用户必须有无密码使用 sudo 的权限。 2.2.1 安装 NTP我们建议在所有 Ceph 节点上安装 NTP 服务（特别是 Ceph Monitor 节点），以免因时钟漂移导致故障，详情见时钟。 1sudo yum install ntp ntpdate ntp-doc 确保在各 Ceph 节点上启动了 NTP 服务，并且要使用同一个 NTP 服务器，详情见 NTP 。 2.2.2 安装 SSH 服务器在所有 Ceph 节点上执行如下步骤： 在各 Ceph 节点安装 SSH 服务器（如果还没有） 1sudo yum install openssh-server 确保所有 Ceph 节点上的 SSH 服务器都在运行。 2.2.3 安装ceph由于蚂蚁内部物理机不能访问外网，使用以下步骤安装ceph。 在所有Ceph节点上执行如下步骤： 下载ceph所有的依赖rpm，并解压缩 12sudo wget http://qianli-lzh.oss-cn-hangzhou-zmf.aliyuncs.com/bill_inference_public%2Fceph.tarsudo tar -xvf bill_inference_public%2Fceph.tar 手动安装所有的rpm 1sudo rpm -ivh --force --nodeps ceph/*.rpm 验证ceph是否正确安装 12ceph -vceph version 12.2.8 (ae699615bac534ea496ee965ac6192cb7e0e07c0) luminous (stable) 2.2.4 关闭防火墙1234sudo sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/configsudo setenforce 0sudo systemctl stop firewalld sudo systemctl disable firewalld 3、集群搭建3.1 搭建Mon集群 (使用admin账户)创建配置文件 在每台节点机器上创建配置文件/etc/ceph/ceph.conf： 123456789101112131415161718192021222324252627[global]fsid = 932XXXXX-fba7-XXXX-9526-a858c613f468mon initial members = e15p13447.ew9mon host = 192.168.226.20,192.168.226.22,192.168.226.96rbd default features = 1auth_cluster_required = noneauth_service_required = noneauth_client_required = nonepublic network = 192.168.226.0/24cluster network = 192.168.226.0/24osd journal size = 1024osd pool default size = 2osd pool default min size = 1osd pool default pg num = 128osd pool default pgp num = 128osd crush chooseleaf type = 1mon_max_pg_per_osd = 200[mds.ceph-1]host = ceph-1[mds.ceph-2]host = ceph-2[mds.ceph-3]host = ceph-3[mon]mon allow pool delete = true 其中 fsid 是为集群分配的一个 uuid, 初始化 mon 节点其实只需要这一个配置就够了。mon host 配置 ceph 命令行工具访问操作 ceph 集群时查找 mon 节点入口。ceph 集群可包含多个 mon 节点实现高可用容灾, 避免单点故障。rbd default features = 1 配置 rbd 客户端创建磁盘时禁用一些需要高版本内核才能支持的特性。 3.1.2 主mon节点 （192.168.226.20）1、为此集群创建密钥环、并生成Monitor密钥 (3台机器一样) 1sudo ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *' 2、生成管理员密钥环，生成 client.admin 用户并加入密钥环 (3台机器一样) 1sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' 3、把 client.admin 密钥加入 ceph.mon.keyring (3台机器一样) 1sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring 4、用规划好的主机名、对应 IP 地址、和 FSID 生成一个Monitor Map，并保存为 /tmp/monmap 12host_name=`hostname`sudo monmaptool --create --add $host_name 192.168.226.20 --fsid 932XXXXX-fba7-XXXX-9526-a858c613f468 /tmp/monmap --clobber 5、在Monitor主机上分别创建数据目录 123host_name=`hostname`#在admin账户下sudo mkdir /var/lib/ceph/mon/ceph-$host_name/ 6、用Monitor Map和密钥环组装守护进程所需的初始数据 1sudo ceph-mon --mkfs -i $host_name --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 7、建一个空文件 done ，表示监视器已创建、可以启动了 1sudo touch /var/lib/ceph/mon/ceph-$host_name/done 8、启动Monitor 1234#sudo ceph-mon -f --cluster ceph --id $host_name &amp;sudo cp /usr/lib/systemd/system/ceph-mon@.service /usr/lib/systemd/system/ceph-mon@$host_name.servicesudo systemctl start ceph-mon@$host_namesudo systemctl enable ceph-mon@$host_name 9、确认下集群在运行 1ceph -s 事例： 1234567891011121314cluster: id: 932XXXXX-fba7-XXXX-9526-a858c613f468 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 0B used, 0B / 0B avail pgs: 3.1.2 从mon节点 (192.168.226.22 &amp; 192.168.226.96)12345678910host_name=`hostname`sudo ceph mon getmap -o /tmp/monmapsudo rm -rf /var/lib/ceph/mon/ceph-$host_namesudo ceph-mon -i $host_name --mkfs --monmap /tmp/monmapsudo chown -R ceph:ceph /var/lib/ceph/mon/ceph-$host_name/#nohup ceph-mon -f --cluster ceph --id $host_name --setuser ceph --setgroup ceph &amp;#ceph-mon -f --cluster ceph --id $host_name &amp;sudo cp /usr/lib/systemd/system/ceph-mon@.service /usr/lib/systemd/system/ceph-mon@$host_name.servicesudo systemctl start ceph-mon@$host_namesudo systemctl enable ceph-mon@$host_name 3.2 创建ceph-mgr3.2.1 创建用户 openstack 用于 MGR 监控1234ceph auth get-or-create mgr.openstack mon 'allow *' osd 'allow *' mds 'allow *'输出：[mgr.openstack] key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxugvXkLfgauLA== 需要将之前创建的用户密码存放至对应位置 123mkdir /var/lib/ceph/mgr/ceph-openstackceph auth get mgr.openstack -o /var/lib/ceph/mgr/ceph-openstack/keyringexported keyring for mgr.openstack 3.2.2 启动mgr1ceph-mgr -i openstack 监控状态 12345678910111213141516$ceph -s cluster: id: 932e88a6-fba7-45a9-9526-a858c613f468 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 mgr: openstack(active) mds: cephfs-1/1/1 up &#123;0=2=up:active&#125;, 2 up:standby osd: 3 osds: 3 up, 3 in data: pools: 2 pools, 256 pgs objects: 21 objects, 3.04KiB usage: 3.32GiB used, 1.17TiB / 1.17TiB avail pgs: 256 active+clean 当 mgr 服务被激活之后, service 中 mgr 会显示 mgr-$name(active)data 部分信息将变得可用 3.3 手动搭建osd集群(三台机器上做相同的操作，注意osd_id的变化)添加一个新osd，id可以省略，ceph会自动使用最小可用整数，第一个osd从0开始 123#ceph osd create &#123;id&#125;ceph osd create0 3.3.1 初始化osd目录创建osd.0目录，目录名格式{cluster-name}-{id} 12#mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;id&#125;sudo mkdir /var/lib/ceph/osd/ceph-0 挂载osd.0的数据盘/dev/sdb2 12sudo mkfs.xfs /dev/sdb2sudo mount /dev/sdb2 /var/lib/ceph/osd/ceph-0 初始化osd数据目录 1234# sudo ceph-osd -i &#123;id&#125; --mkfs --mkkeysudo ceph-osd -i 0 --mkfs --mkkey#--mkkey要求osd数据目录为空#这会创建osd.0的keyring /var/lib/ceph/osd/ceph-0/keyring 初始化后，默认使用普通文件/var/lib/ceph/osd/ceph-3/journal作为osd.0的journal分区，普通文件作为journal分区性能不高，若只是测试环境，可以跳过更改journal分区这一步骤 3.3.2 创建journal生成journal分区，一般选ssd盘作为journal分区，这里使用ssd的/dev/sdb1分区作为journal 使用fdisk工分出磁盘/dev/sdb1, 1234567891011121314151617181920#清除磁盘所有分区(重新添加时需要)#sgdisk --zap-all --clear --mbrtogpt /dev/sdb#生成分区/dev/sdb1的uuid#uuidgen#b3897364-8807-48eb-9905-e2c8400d0cd4#创建分区#1:0:+100G 表示创建第一个分区，100G大小#sudo sgdisk --new=1:0:+100G --change-name=1:'ceph journal' --partition-guid=1:b3897364-8807-48eb-9905-e2c8400d0cd4 --typecode=1:b3897364-8807-48eb-9905-e2c8400d0cd4 --mbrtogpt -- /dev/vdf#格式化sudo mkfs.xfs /dev/sdb1sudo rm -f /var/lib/ceph/osd/ceph-4/journal #查看分区对应的partuuid， 找出/dev/sdb1对应的partuuidsudo blkidsudo ln -s /dev/disk/by-partuuid/b3897364-8807-48eb-9905-e2c8400d0cd4 /var/lib/ceph/osd/ceph-0/journalsudo chown ceph:ceph -R /var/lib/ceph/osd/ceph-0sudo chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal#初始化新的journalsudo ceph-osd --mkjournal -i 0sudo chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal 3.3.3 注册osd.{id}，id为osd编号，默认从0开始123# sudo ceph auth add osd.&#123;id&#125; osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-&#123;id&#125;/keyringsudo ceph auth add osd.0 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-0/keyring#ceph auth list 中出现osd.0 3.3.4 加入crush map这是m1上新创建的第一个osd，CRUSH map中还没有m1节点，因此首先要把m1节点加入CRUSH map，同理，m2/m3节点也需要加入CRUSH map 12#ceph osd crush add-bucket &#123;hostname&#125; hostsudo ceph osd crush add-bucket `hostname` host 然后把三个节点移动到默认的root default下面 1sudo ceph osd crush move `hostname` root=default 添加osd.0到CRUSH map中的m1节点下面，加入后，osd.0就能够接收数据 123#ceph osd crush add osd.&#123;id&#125; 0.4 root=sata rack=sata-rack01 host=sata-node5sudo ceph osd crush add osd.4 1.7 root=default host=`hostname`#0.4为此osd在CRUSH map中的权重值，它表示数据落在此osd上的比重，是一个相对值，一般按照1T磁盘比重值为1来计算，这里的osd数据盘1.7，所以值为1.7 此时osd.0状态是down且in，in表示此osd位于CRUSH map，已经准备好接受数据，down表示osd进程运行异常，因为我们还没有启动osd.0进程 3.3.5 启动ceph-osd进程需要向systemctl传递osd的id以启动指定的osd进程，如下，我们准备启动osd.0进程 12345#systemctl start ceph-osd@&#123;id&#125; id表示osd编号，从数字0开始sudo cp /usr/lib/systemd/system/ceph-osd@.service /usr/lib/systemd/system/ceph-osd@0.servicesudo systemctl start ceph-osd@0sudo systemctl enable ceph-osd@0#sudo ceph-osd -i 0 上面就是添加osd.0的步骤，然后可以接着在其他hostname节点上添加osd.{1,2}，添加了这3个osd后，可以查看集群状态 ceph -s。 3.4 搭建MDS创建目录： 12sudo mkdir /var/lib/ceph/mds/ceph-`hostname`sudo chown ceph:ceph -R /var/lib/ceph/mds/ceph-`hostname` 在ceph.conf中添加如下信息： 12345[mds.&#123;id&#125;]host = &#123;id&#125;例如：[mds.0]host = 0 启动mds 12345#ceph-mds --cluster &#123;cluster-name&#125; -i &#123;id&#125; -m &#123;mon-hostname&#125;:&#123;mon-port&#125; [-f]sudo cp /usr/lib/systemd/system/ceph-mds@.service /usr/lib/systemd/system/ceph-mds@`hostname`.service sudo systemctl start ceph-mds@`hostname`sudo systemctl enable ceph-mds@`hostname`#ceph-mds --cluster ceph -i 0 -m e15p13447.ew9:6789 查看mds状态 12ceph mds statcephfs-1/1/1 up &#123;0=1=up:active&#125;, 2 up:standby 至此ceph集群搭建完成。","categories":[{"name":"ceph","slug":"ceph","permalink":"https://www.zuoyangblog.com/categories/ceph/"}],"tags":[{"name":"ceph","slug":"ceph","permalink":"https://www.zuoyangblog.com/tags/ceph/"}]}]}