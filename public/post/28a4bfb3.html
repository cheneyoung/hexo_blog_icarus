<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    
    <title>第三篇：手动部署Ceph集群 | zuoyang&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="上篇介绍了部署Ceph的环境准备工作，本篇主要内容是介绍手动部署Ceph的全过程。 环境 集群环境 上一篇，我们创建了3台虚拟机，虚拟机配置如下： 12345678910111213[root@ceph-1 ~]# lsblkNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0  100G  0">
<meta name="keywords" content="ceph,ceph-mon,ceph-osd,ceph-mgr">
<meta property="og:type" content="article">
<meta property="og:title" content="第三篇：手动部署Ceph集群">
<meta property="og:url" content="https://www.zuoyangblog.com/post/28a4bfb3.html">
<meta property="og:site_name" content="zuoyang&#39;s blog">
<meta property="og:description" content="上篇介绍了部署Ceph的环境准备工作，本篇主要内容是介绍手动部署Ceph的全过程。 环境 集群环境 上一篇，我们创建了3台虚拟机，虚拟机配置如下： 12345678910111213[root@ceph-1 ~]# lsblkNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0  100G  0">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://www.zuoyangblog.com/post/28a4bfb3/disk_part.jpg">
<meta property="og:updated_time" content="2018-11-22T15:38:43.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第三篇：手动部署Ceph集群">
<meta name="twitter:description" content="上篇介绍了部署Ceph的环境准备工作，本篇主要内容是介绍手动部署Ceph的全过程。 环境 集群环境 上一篇，我们创建了3台虚拟机，虚拟机配置如下： 12345678910111213[root@ceph-1 ~]# lsblkNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0  100G  0">
<meta name="twitter:image" content="https://www.zuoyangblog.com/post/28a4bfb3/disk_part.jpg">
    

    

    
        <link rel="icon" href="../css/images/horse.jpg">
    

    <link rel="stylesheet" href="../libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="../libs/open-sans/styles.css">
    <link rel="stylesheet" href="../libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="../css/style.css">

    <script src="../libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="../libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="../libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129296219-1', 'auto');
ga('send', 'pageview');

</script>
    
    
    


</head>
</html>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="../index.html" id="logo">
                <i class="logo"></i>
                <span class="site-title">zuoyang&#39;s blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="../.">Home</a>
                
                    <a class="main-nav-link" href="../archives">Archives</a>
                
                    <a class="main-nav-link" href="../categories">Categories</a>
                
                    <a class="main-nav-link" href="../tags">Tags</a>
                
                    <a class="main-nav-link" href="../about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="../css/images/profile_pic.jpg">
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '../content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="../js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="../.">Home</a></td>
                
                    <td><a class="main-nav-link" href="../archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="../categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="../tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="../about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <script type='text/javascript'>
                $('body').append("<style type='text/css'>@media screen and (min-width: 1200px) { #main {margin: auto; width: 70%; display: block; float:none;}}</style>");
                </script>
                <script type='text/javascript'>
                document.querySelector('style').textContent += "@media screen and (max-width: 1199px) and (min-width: 800px) { #main {margin: auto; width: 70%; display: block; float:none;}}"
                </script>
            
            <section id="main"><article id="post-manual-deploy-ceph" class="article article-type-post" itemscope="" itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name" style="color: #009688;font-size:1.5em;">
            第三篇：手动部署Ceph集群
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="../categories/ceph/">ceph</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="../tags/ceph/">ceph</a>, <a class="tag-link" href="../tags/ceph-mgr/">ceph-mgr</a>, <a class="tag-link" href="../tags/ceph-mon/">ceph-mon</a>, <a class="tag-link" href="../tags/ceph-osd/">ceph-osd</a>
    </div>

                        
                            <i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span>次
                        
                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">Catalogue</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">环境</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.1.</span> <span class="toc-text">集群环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.</span> <span class="toc-text">配置NTP服务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.1.</span> <span class="toc-text">修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.2.</span> <span class="toc-text">关闭防火墙</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.3.</span> <span class="toc-text">启动ntp服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.4.</span> <span class="toc-text">配置客户端ntp同步</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">手动搭建ceph集群</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.</span> <span class="toc-text">手动部署mon集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.1.</span> <span class="toc-text">主mon节点部署 （192.168.56.101，ceph-1）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.2.</span> <span class="toc-text">从mon节点部署 (192.168.56.102 &amp; 192.168.56.103)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">创建启动脚本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">启动mon进程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.</span> <span class="toc-text">手动搭建ceph-mgr</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.1.</span> <span class="toc-text">创建 MGR 监控用户</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.2.</span> <span class="toc-text">启动mgr</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.3.</span> <span class="toc-text">验证是否成功</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.4.</span> <span class="toc-text">脚本操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.3.</span> <span class="toc-text">手动搭建osd集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.3.1.</span> <span class="toc-text">磁盘分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.3.2.</span> <span class="toc-text">启动osd进程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.4.</span> <span class="toc-text">手动搭建MDS (仅cephfs使用)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">Ceph集群搭建完成</span></a></li></ol>
                </div>
            
        
        
            <p>上篇介绍了部署Ceph的环境准备工作，本篇主要内容是介绍手动部署Ceph的全过程。</p>
<h1><span id="环境">环境</span></h1>
<h2><span id="集群环境">集群环境</span></h2>
<p><a href="https://www.zuoyangblog.com/post/ad293d8.html">上一篇</a>，我们创建了3台虚拟机，虚拟机配置如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0  100G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0    2G  0 lvm  [SWAP]</span><br><span class="line">  └─centos-home 253:2    0   47G  0 lvm  /home</span><br><span class="line">sdb               8:16   0    2T  0 disk</span><br><span class="line">sdc               8:32   0    2T  0 disk</span><br><span class="line">sdd               8:48   0    2T  0 disk</span><br><span class="line">sde               8:64   0  600G  0 disk</span><br><span class="line">sr0              11:0    1 1024M  0 rom</span><br></pre></td></tr></table></figure>
<ul>
<li>3块大小为2T的磁盘，sdb,sdc,sdd，用于启动osd进程</li>
<li>一块大小为600G的磁盘，作为osd的journal分区</li>
</ul>
<p><strong>集群配置如下：</strong></p>
<table>
<thead>
<tr>
<th>主机</th>
<th>IP</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>ceph-1</td>
<td>192.168.56.101</td>
<td>mon1、osd0、osd1、osd2</td>
</tr>
<tr>
<td>ceph-2</td>
<td>192.168.56.102</td>
<td>mon2、osd3、osd4、osd5</td>
</tr>
<tr>
<td>ceph-3</td>
<td>192.168.56.103</td>
<td>mon3、osd6、osd7、osd8</td>
</tr>
</tbody>
</table>
<h2><span id="配置ntp服务">配置NTP服务</span></h2>
<p>将NTP server放在ceph-1节点上，ceph-2/3节点是NTP client，这样可以从根本上解决时间同步问题。</p>
<h3><span id="修改配置文件">修改配置文件</span></h3>
<p>从本机登录到ceph-1：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh root@192.168.56.101</span><br></pre></td></tr></table></figure>
<p>在ceph-1节点上进行如下操作：</p>
<p>修改<code>/etc/ntp.conf</code>,注释掉默认的四个server，添加三行配置如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim  /etc/ntp.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##comment following lines:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##add following lines:</span></span></span><br><span class="line">server 127.127.1.0 minpoll 4</span><br><span class="line">fudge 127.127.1.0 stratum 0</span><br><span class="line">restrict 192.168.56.0 mask 255.255.0.0 nomodify notrap</span><br></pre></td></tr></table></figure>
<p>修改<code>/etc/ntp/step-tickers</code>文件如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> List of NTP servers used by the ntpdate service.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 0.centos.pool.ntp.org</span></span><br><span class="line">127.127.1.0</span><br></pre></td></tr></table></figure>
<p>在重启ntp服务之前需要将防火墙关闭，否则客户端不能访问ntp服务：</p>
<h3><span id="关闭防火墙">关闭防火墙</span></h3>
<p>关闭<code>selinux</code>&amp;<code>firewalld</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line">systemctl stop firewalld </span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>
<h3><span id="启动ntp服务">启动ntp服务</span></h3>
<p>重启ntp服务，并查看server端是否运行正常，正常的标准就是<code>ntpq -p</code>指令的最下面一行是<code>*</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# systemctl enable ntpd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.</span><br><span class="line">[root@ceph-1 ~]# systemctl restart ntpd </span><br><span class="line">[root@ceph-1 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line">*LOCAL(0)        .LOCL.           0 l   15   16    1    0.000    0.000   0.000</span><br></pre></td></tr></table></figure>
<p>NTP server端已经配置完毕，下面开始配置client端。</p>
<h3><span id="配置客户端ntp同步">配置客户端ntp同步</span></h3>
<p>同样的方式登录到<strong>ceph-2/ceph-3</strong>机器上：</p>
<p>修改<code>/etc/ntp.conf</code>，注释掉四行server，添加一行server指向ceph-1:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ntp.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"></span><br><span class="line">server 192.168.56.101</span><br></pre></td></tr></table></figure>
<p>重启ntp服务并观察client是否正确连接到server端，同样正确连接的标准是<code>ntpq -p</code>的最下面一行以<code>*</code>号开头:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ~]# systemctl stop firewalld</span><br><span class="line">[root@ceph-2 ~]# systemctl disable firewalld </span><br><span class="line">[root@ceph-2 ~]# systemctl restart ntpd</span><br><span class="line">[root@ceph-2 ~]# systemctl enable ntpd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.</span><br></pre></td></tr></table></figure>
<p>开始的时候会显示INIT状态，然后等了几分钟之后就出现了<code>*</code>。</p>
<p>异常状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> ceph-1          .INIT.          16 u    -   64    0    0.000    0.000   0.000</span><br></pre></td></tr></table></figure>
<p>正常状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ~]# ntpq -p</span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line">*ceph-1          LOCAL(0)         3 u   45   64   77    0.323    0.060   0.034</span><br></pre></td></tr></table></figure>
<p><strong>在搭建ceph集群之前，一定要保证ntp服务能够正常运行。</strong></p>
<h1><span id="手动搭建ceph集群">手动搭建ceph集群</span></h1>
<h2><span id="手动部署mon集群">手动部署mon集群</span></h2>
<h3><span id="主mon节点部署-19216856101ceph-1">主mon节点部署 （192.168.56.101，ceph-1）</span></h3>
<p>登录到ceph-1机器：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh root@192.168.56.101</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>创建ceph用户与目录</strong></li>
</ul>
<p>新建一个脚本文件<code>prepare_env.sh</code>,然后写入以下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# vim prepare_env.sh</span><br><span class="line">/usr/sbin/groupadd ceph -g 167 -o -r</span><br><span class="line">/usr/sbin/useradd ceph -u 167 -o -r -g ceph -s /sbin/nologin -c "Ceph daemons"</span><br><span class="line">mkdir -p /etc/ceph/</span><br><span class="line">chown -R ceph:ceph /etc/ceph/</span><br><span class="line">mkdir -p /var/run/ceph</span><br><span class="line">chown -R ceph:ceph /var/run/ceph</span><br><span class="line">mkdir -p /var/log/ceph</span><br><span class="line">chown -R ceph:ceph /var/log/ceph</span><br><span class="line">mkdir -p /var/lib/ceph/mon</span><br><span class="line">chown -R ceph:ceph /var/lib/ceph</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>创建ceph.conf文件</strong></li>
</ul>
<p>新建一个conf文件<code>ceph.conf</code>,然后写入以下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# vim /etc/ceph/ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">mon initial members = ceph-1</span><br><span class="line">mon host = 192.168.56.101,192.168.56.102,192.168.56.103</span><br><span class="line">rbd default features = 1</span><br><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br><span class="line">public network = 192.168.56.0/24</span><br><span class="line">cluster network = 192.168.56.0/24</span><br><span class="line">osd journal size = 1024</span><br><span class="line">osd pool default size = 3</span><br><span class="line">osd pool default min size = 1</span><br><span class="line">osd pool default pg num = 300</span><br><span class="line">osd pool default pgp num = 300</span><br><span class="line">osd crush chooseleaf type = 1</span><br><span class="line"></span><br><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true</span><br></pre></td></tr></table></figure>
<p>这里不对各个参数的含义进行解释，对于初学者而言，先把集群搭建起来，后面再去花时间了解整个ceph的原理及配置文件各个参数对集群的影响。</p>
<p>其中 <code>fsid</code> 是为集群分配的一个 uuid, 可使用<code>uuidgen</code>命令生成。初始化 mon 节点其实只需要这一个配置就够了。<br>
<code>mon host</code> 配置 ceph 命令行工具访问操作 ceph 集群时查找 mon 节点入口。<br>
ceph 集群可包含多个 mon 节点实现高可用容灾, 避免单点故障。<br>
<code>rbd default features = 1</code> 配置 rbd 客户端创建磁盘时禁用一些需要高版本内核才能支持的特性。</p>
<ul>
<li><strong>拷贝这两个文件到ceph-2,ceph-3机器上</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp prepare_env.sh  192.168.56.102:/home</span><br><span class="line">scp prepare_env.sh  192.168.56.103:/home</span><br><span class="line">scp /etc/ceph/ceph.conf  192.168.56.102:/etc/ceph/</span><br><span class="line">scp /etc/ceph/ceph.conf  192.168.56.103:/etc/ceph/</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>启动mon节点</strong></li>
</ul>
<p>首先执行脚本,创建ceph用户及相关目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh prepare_env.sh</span><br></pre></td></tr></table></figure>
<p>1、为此集群创建密钥环、并生成Monitor密钥</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'</span><br><span class="line">creating /tmp/ceph.mon.keyring</span><br></pre></td></tr></table></figure>
<p>2、生成管理员密钥环，生成 <code>client.admin</code> 用户并加入密钥环</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'</span><br><span class="line">creating /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure>
<p>3、把 <code>client.admin</code> 密钥加入 <code>ceph.mon.keyring</code>  (3台机器一样)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">importing contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring</span><br></pre></td></tr></table></figure>
<p>4、用规划好的主机名、对应 IP 地址、和 FSID 生成一个Monitor Map，并保存为 <code>/tmp/monmap</code></p>
<p>这里的<code>--fsid</code>需要跟ceph.conf里面的<code>fsid</code>保持一致</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# monmaptool --create --add `hostname` `hostname -i`  --fsid c165f9d0-88df-48a7-8cc5-11da82f99c93 /tmp/monmap --clobber</span><br><span class="line">monmaptool: monmap file /tmp/monmap</span><br><span class="line">monmaptool: set fsid to c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">monmaptool: writing epoch 0 to /tmp/monmap (1 monitors)</span><br></pre></td></tr></table></figure>
<p>5、在Monitor主机上分别创建数据目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# rm -rf /var/lib/ceph/mon/ceph-`hostname`/</span><br><span class="line">[root@ceph-1 ~]# mkdir /var/lib/ceph/mon/ceph-`hostname`/</span><br><span class="line">[root@ceph-1 ~]# chown ceph:ceph -R /var/lib/ceph/mon/ceph-`hostname`/</span><br><span class="line">[root@ceph-1 ~]# chown -R ceph:ceph /var/lib/ceph/</span><br><span class="line">[root@ceph-1 ~]# chown  ceph:ceph /tmp/monmap</span><br><span class="line">[root@ceph-1 ~]# chown  ceph:ceph /tmp/ceph.mon.keyring</span><br></pre></td></tr></table></figure>
<p>6、用Monitor Map和密钥环组装守护进程所需的初始数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# sudo -u ceph ceph-mon --mkfs -i `hostname` --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</span><br></pre></td></tr></table></figure>
<p>7、建一个空文件 <code>done</code> ，表示监视器已创建、可以启动了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# touch /var/lib/ceph/mon/ceph-`hostname`/done</span><br></pre></td></tr></table></figure>
<p>8、启动Monitor</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ~]# cp /usr/lib/systemd/system/ceph-mon@.service /usr/lib/systemd/system/ceph-mon@`hostname`.service</span><br><span class="line">[root@ceph-1 ~]# sudo systemctl start ceph-mon@`hostname`</span><br><span class="line">[root@ceph-1 ~]# sudo systemctl enable ceph-mon@`hostname`</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@ceph-1.service to /usr/lib/systemd/system/ceph-mon@ceph-1.service.</span><br></pre></td></tr></table></figure>
<p>9、确认下集群在运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph -s</span><br><span class="line"> cluster:</span><br><span class="line">   id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">   health: HEALTH_OK</span><br><span class="line"></span><br><span class="line"> services:</span><br><span class="line">   mon: 1 daemons, quorum ceph-1</span><br><span class="line">   mgr: no daemons active</span><br><span class="line">   osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line"> data:</span><br><span class="line">   pools:   0 pools, 0 pgs</span><br><span class="line">   objects: 0 objects, 0B</span><br><span class="line">   usage:   0B used, 0B / 0B avail</span><br><span class="line">   pgs:</span><br></pre></td></tr></table></figure>
<h3><span id="从mon节点部署-19216856102-amp-19216856103">从mon节点部署 (192.168.56.102 &amp; 192.168.56.103)</span></h3>
<p>在另外两台机器ceph-2,ceph-3上，分别ssh登录上去。</p>
<h4><span id="创建启动脚本">创建启动脚本</span></h4>
<p>新建启动脚本文件<code>start_repo_mon.sh</code></p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> start_repo_mon.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">host_name=`hostname`</span><br><span class="line">sudo ceph mon getmap -o /tmp/monmap</span><br><span class="line">sudo rm -rf /var/lib/ceph/mon/ceph-$host_name</span><br><span class="line">sudo ceph-mon -i $host_name --mkfs --monmap /tmp/monmap</span><br><span class="line">sudo chown -R ceph:ceph /var/lib/ceph/mon/ceph-$host_name/</span><br><span class="line"><span class="meta">#</span><span class="bash">nohup ceph-mon -f --cluster ceph --id <span class="variable">$host_name</span> --setuser ceph --setgroup ceph &amp;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">ceph-mon -f --cluster ceph --id <span class="variable">$host_name</span> &amp;</span></span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-mon@.service /usr/lib/systemd/system/ceph-mon@$host_name.service</span><br><span class="line">sudo systemctl start ceph-mon@$host_name</span><br><span class="line">sudo systemctl enable ceph-mon@$host_name</span><br></pre></td></tr></table></figure>
<h4><span id="启动mon进程">启动mon进程</span></h4>
<ul>
<li><strong>ceph-2机器192.168.56.102</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ceph]# sh prepare_env.sh</span><br><span class="line">groupadd：“ceph”组已存在</span><br><span class="line">useradd：用户“ceph”已存在</span><br><span class="line">[root@ceph-2 ceph]# sh start_repo_mon.sh</span><br><span class="line">got monmap epoch 1</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@ceph-2.service to /usr/lib/systemd/system/ceph-mon@ceph-2.service.</span><br><span class="line">[root@ceph-2 ceph]# ps -ef | grep ceph</span><br><span class="line">ceph       11852       1  0 20:47 ?        00:00:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-2 --setuser ceph --setgroup ceph</span><br><span class="line">root       11908   11645  0 20:47 pts/0    00:00:00 grep --color=auto ceph</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ceph]# ceph -s</span><br><span class="line">  id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 2 daemons, quorum ceph-1,ceph-2</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ceph-3机器192.168.56.103</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-3 ceph]# sh prepare_env.sh</span><br><span class="line">groupadd：“ceph”组已存在</span><br><span class="line">useradd：用户“ceph”已存在</span><br><span class="line">[root@ceph-3 ceph]# sh start_repo_mon.sh</span><br><span class="line">got monmap epoch 2</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@ceph-3.service to /usr/lib/systemd/system/ceph-mon@ceph-3.service.</span><br><span class="line">[root@ceph-3 ceph]# ps -ef | grep ceph</span><br><span class="line">ceph       11818       1  1 20:51 ?        00:00:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-3 --setuser ceph --setgroup ceph</span><br><span class="line">root       11874   11081  0 20:51 pts/0    00:00:00 grep --color=auto ceph</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-3 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>至此可以看到ceph mon集群搭建完毕。</p>
<h2><span id="手动搭建ceph-mgr">手动搭建ceph-mgr</span></h2>
<p>首先在ceph-1机器上创建并启动ceph-mgr进程。</p>
<h3><span id="创建-mgr-监控用户">创建 MGR 监控用户</span></h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph auth get-or-create mgr.`hostname` mon 'allow *' osd 'allow *' mds 'allow *'</span><br><span class="line">[mgr.ceph-1]</span><br><span class="line">	key = AQCvovZbpUHTDBAA+/RoCVv+GTBc7lb96rOXRg==</span><br></pre></td></tr></table></figure>
<p>需要将之前创建的用户密码存放至对应位置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# mkdir /var/lib/ceph/mgr/ceph-`hostname`</span><br><span class="line">[root@ceph-1 ceph]# ceph auth get mgr.`hostname` -o  /var/lib/ceph/mgr/ceph-`hostname`/keyring</span><br><span class="line">exported keyring for mgr.ceph-1</span><br></pre></td></tr></table></figure>
<h3><span id="启动mgr">启动mgr</span></h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# cp /usr/lib/systemd/system/ceph-mgr@.service /usr/lib/systemd/system/ceph-mgr@`hostname`.service</span><br><span class="line">[root@ceph-1 ceph]# systemctl start ceph-mgr@`hostname`</span><br><span class="line">[root@ceph-1 ceph]# systemctl enable ceph-mgr@`hostname`</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph-1.service to /usr/lib/systemd/system/ceph-mgr@ceph-1.service.</span><br></pre></td></tr></table></figure>
<h3><span id="验证是否成功">验证是否成功</span></h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# systemctl status ceph-mgr@`hostname`</span><br><span class="line">● ceph-mgr@ceph-1.service - Ceph cluster manager daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-mgr@ceph-1.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 四 2018-11-22 07:38:49 EST; 14s ago</span><br><span class="line"> Main PID: 2050 (ceph-mgr)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dmgr.slice/ceph-mgr@ceph-1.service</span><br><span class="line">           └─2050 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">11月 22 07:38:49 ceph-1 systemd[1]: Started Ceph cluster manager daemon.</span><br><span class="line">11月 22 07:38:49 ceph-1 systemd[1]: Starting Ceph cluster manager daemon...</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ps -ef | grep ceph-mgr</span><br><span class="line">ceph        2050       1  1 07:38 ?        00:00:00 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-1 --setuser ceph --setgroup ceph</span><br><span class="line">root        2102    1249  0 07:39 pts/0    00:00:00 grep --color=auto ceph-mgr</span><br></pre></td></tr></table></figure>
<p>说明mgr进程正常启动。</p>
<p>监控状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: ceph-1(active), standbys: admin</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>当 mgr 服务被激活之后, service 中 mgr 会显示 mgr-$name(active)<br>
data 部分信息将变得可用</p>
<h3><span id="脚本操作">脚本操作</span></h3>
<p>在ceph-2,ceph-3机器上，创建脚本<code>start_mgr.sh</code>，写入以下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ceph]# vim start_mgr.sh</span><br><span class="line">host_name=`hostname`</span><br><span class="line">sudo ceph auth get-or-create mgr.$host_name mon 'allow *' osd 'allow *' mds 'allow *'</span><br><span class="line">sudo rm -rf /var/lib/ceph/mgr/ceph-$host_name</span><br><span class="line">sudo mkdir /var/lib/ceph/mgr/ceph-$host_name</span><br><span class="line">sudo ceph auth get mgr.$host_name -o  /var/lib/ceph/mgr/ceph-$host_name/keyring</span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-mgr@.service /usr/lib/systemd/system/ceph-mgr@$host_name.service</span><br><span class="line">sudo systemctl start ceph-mgr@$host_name</span><br><span class="line">sudo systemctl enable ceph-mgr@$host_name</span><br></pre></td></tr></table></figure>
<p>执行脚本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ceph]# sh start_mgr.sh</span><br><span class="line">[mgr.ceph-2]</span><br><span class="line">	key = AQCTpfZbNLmpFxAACe1gMNUM4vqKMfNdUGbY/A==</span><br><span class="line">exported keyring for mgr.ceph-2</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph-2.service to /usr/lib/systemd/system/ceph-mgr@ceph-2.service.</span><br></pre></td></tr></table></figure>
<p>查看进程状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ceph]# ps -ef | grep ceph-mgr</span><br><span class="line">ceph       12101       1  0 07:48 ?        00:00:00 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-2 --setuser ceph --setgroup ceph</span><br><span class="line">root       12173   11645  0 07:50 pts/0    00:00:00 grep --color=auto ceph-mgr</span><br><span class="line">[root@ceph-2 ceph]# systemctl status ceph-mgr*</span><br><span class="line">● ceph-mgr@ceph-2.service - Ceph cluster manager daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-mgr@ceph-2.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 四 2018-11-22 07:48:19 EST; 2min 33s ago</span><br><span class="line"> Main PID: 12101 (ceph-mgr)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dmgr.slice/ceph-mgr@ceph-2.service</span><br><span class="line">           └─12101 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-2 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">11月 22 07:48:19 ceph-2 systemd[1]: Started Ceph cluster manager daemon.</span><br><span class="line">11月 22 07:48:19 ceph-2 systemd[1]: Starting Ceph cluster manager daemon...</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-2 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: ceph-1(active), standbys: admin, ceph-2</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>可以看到在ceph-2机器上，ceph-mgr进程正常启动。</p>
<p>最后再ceph-3上进行同样的操作。</p>
<p>所有mgr进程创建完成之后，集群的状态如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-3 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: ceph-1(active), standbys: admin, ceph-2, ceph-3</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<ul>
<li>health是处于OK状态</li>
<li>3个mon daemons</li>
<li>mgr进程：ceph-1是active状态，剩下的ceph-2,ceph-3处于standby</li>
</ul>
<h2><span id="手动搭建osd集群">手动搭建osd集群</span></h2>
<h3><span id="磁盘分区">磁盘分区</span></h3>
<p>每个osd对应一块磁盘，是ceph集群存储数据的物理单位，在搭建osd集群之前，先要对三台机器的磁盘进行处理，这里每台机器都是相同的操作，这里只演示ceph-1上的操作：</p>
<ul>
<li>查看磁盘分布情况</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0  100G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0    2G  0 lvm  [SWAP]</span><br><span class="line">  └─centos-home 253:2    0   47G  0 lvm  /home</span><br><span class="line">sdb               8:16   0    2T  0 disk</span><br><span class="line">sdc               8:32   0    2T  0 disk</span><br><span class="line">sdd               8:48   0    2T  0 disk</span><br><span class="line">sde               8:64   0  600G  0 disk</span><br><span class="line">sr0              11:0    1 1024M  0 rom</span><br></pre></td></tr></table></figure>
<p>这里我们可以看到有三块2T的磁盘:<code>sdb,sdc,sdd</code>,用来部署osd;一块600G的磁盘用作每个osd的journal分区。</p>
<ul>
<li>sde进行分区：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# fdisk /dev/sde</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line"></span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line"></span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">使用磁盘标识符 0x986f9840 创建新的 DOS 磁盘标签。</span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：g</span><br><span class="line">Building a new GPT disklabel (GUID: E7350C3E-586A-424E-ABAB-73860654C2C8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (1-128，默认 1)：</span><br><span class="line">第一个扇区 (2048-1258279965，默认 2048)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-1258279965，默认 1258279965)：+200G</span><br><span class="line">已创建分区 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (2-128，默认 2)：</span><br><span class="line">第一个扇区 (419432448-1258279965，默认 419432448)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (419432448-1258279965，默认 1258279965)：+200G</span><br><span class="line">已创建分区 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (3-128，默认 3)：</span><br><span class="line">第一个扇区 (838862848-1258279965，默认 838862848)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (838862848-1258279965，默认 1258279965)：</span><br><span class="line">已创建分区 3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br></pre></td></tr></table></figure>
<p>sde磁盘分区好之后，如下图所示：</p>
<img src="/post/28a4bfb3/disk_part.jpg">
<ul>
<li>创建xfs文件系统</li>
</ul>
<p>分别对磁盘<code>sdb,sdc,sdd,sde1,sde2,sde3</code>进行如下操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# mkfs.xfs /dev/sdb # 之后将sdb替换为sdc,sdd,sde1,sde2,sde3</span><br><span class="line">meta-data=/dev/sdb               isize=512    agcount=4, agsize=134211328 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=536845310, imaxpct=5</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=262131, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br></pre></td></tr></table></figure>
<p>对这几个磁盘操作完之后，查看每个磁盘的uuid</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# blkid</span><br><span class="line">/dev/sda1: UUID="b66ddda6-8a4c-48d8-bb8a-9a3183094c5d" TYPE="xfs"</span><br><span class="line">/dev/sda2: UUID="Iylyc4-LvIV-TnAg-FZK6-xz2D-3gaY-hx17eb" TYPE="LVM2_member"</span><br><span class="line">/dev/sdb: UUID="104c6fd4-58e7-468f-a003-266d9a7fb9ba" TYPE="xfs"</span><br><span class="line">/dev/sdc: UUID="1a06d53d-0f76-4931-9add-fe3494510edc" TYPE="xfs"</span><br><span class="line">/dev/sdd: UUID="30a5527d-e7b6-4c13-b735-c9e086775d51" TYPE="xfs"</span><br><span class="line">/dev/sde1: UUID="58793fce-298c-417c-85dc-b0d913f8cd63" TYPE="xfs" PARTUUID="ca1a9c40-e020-4bf8-a17b-5716b3e1d453"</span><br><span class="line">/dev/sde2: UUID="1c7ef6a8-58a6-47c0-9d2d-ff086a3d81f7" TYPE="xfs" PARTUUID="c458e242-716d-42f7-9a2e-5f21c291987a"</span><br><span class="line">/dev/sde3: UUID="0581b33a-321d-42af-9b5f-272a775fccc1" TYPE="xfs" PARTUUID="651f90d2-9697-4474-b3d8-a0e980b125a4"</span><br><span class="line">/dev/mapper/centos-root: UUID="0fc63cb7-c2e6-46e8-8db9-2c31fdd20310" TYPE="xfs"</span><br><span class="line">/dev/mapper/centos-swap: UUID="a8605f7e-5049-4cd4-bd6d-805c98543f38" TYPE="swap"</span><br><span class="line">/dev/mapper/centos-home: UUID="7fc267af-b141-4140-afb9-388453097422" TYPE="xfs"</span><br></pre></td></tr></table></figure>
<p>可以看到这些磁盘都有独立的UUID，且TYPE都为“xfs”。说明创建成功。</p>
<h3><span id="启动osd进程">启动osd进程</span></h3>
<ol>
<li>
<p><strong>创建osd id</strong></p>
<p>添加一个新osd，<code>id</code>可以省略，ceph会自动使用最小可用整数，第一个osd从0开始</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph osd create</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>初始化osd目录</strong></p>
<p>创建osd.0目录，目录名格式<code>{cluster-name}-{id}</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;id&#125;</span></span><br><span class="line">[root@ceph-1 ceph]# mkdir /var/lib/ceph/osd/ceph-0</span><br></pre></td></tr></table></figure>
<p>挂载osd.0的数据盘/dev/sdb</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# mount /dev/sdb /var/lib/ceph/osd/ceph-0</span><br></pre></td></tr></table></figure>
<p>查看挂载结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0  100G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0    2G  0 lvm  [SWAP]</span><br><span class="line">  └─centos-home 253:2    0   47G  0 lvm  /home</span><br><span class="line">sdb               8:16   0    2T  0 disk /var/lib/ceph/osd/ceph-0</span><br><span class="line">sdc               8:32   0    2T  0 disk</span><br><span class="line">sdd               8:48   0    2T  0 disk</span><br><span class="line">sde               8:64   0  600G  0 disk</span><br><span class="line">├─sde1            8:65   0  200G  0 part</span><br><span class="line">├─sde2            8:66   0  200G  0 part</span><br><span class="line">└─sde3            8:67   0  200G  0 part</span><br><span class="line">sr0              11:0    1 1024M  0 rom</span><br></pre></td></tr></table></figure>
<p>初始化osd数据目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sudo ceph-osd -i &#123;id&#125; --mkfs --mkkey</span></span><br><span class="line">[root@ceph-1 ceph]# ceph-osd -i 0 --mkfs --mkkey</span><br><span class="line">2018-11-22 08:26:54.255294 7fb2734e6d80 -1 auth: error reading file: /var/lib/ceph/osd/ceph-0/keyring: can't open /var/lib/ceph/osd/ceph-0/keyring: (2) No such file or directory</span><br><span class="line">2018-11-22 08:26:54.257686 7fb2734e6d80 -1 created new key in keyring /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">2018-11-22 08:26:54.306488 7fb2734e6d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</span><br><span class="line">2018-11-22 08:26:54.341905 7fb2734e6d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</span><br><span class="line">2018-11-22 08:26:54.342312 7fb2734e6d80 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">2018-11-22 08:26:54.342327 7fb2734e6d80 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">2018-11-22 08:26:54.342704 7fb2734e6d80 -1 read_settings error reading settings: (2) No such file or directory</span><br><span class="line">2018-11-22 08:26:54.395459 7fb2734e6d80 -1 created object store /var/lib/ceph/osd/ceph-0 for osd.0 fsid c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>创建journal</strong></p>
<p>生成journal分区，一般选ssd盘作为journal分区，这里使用/dev/sde1分区作为osd.0的journal。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看分区对应的partuuid， 找出/dev/sde1对应的partuuid</span></span><br><span class="line">[root@ceph-1 ceph]# rm -f /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[root@ceph-1 ceph]# blkid</span><br><span class="line">/dev/sda1: UUID="b66ddda6-8a4c-48d8-bb8a-9a3183094c5d" TYPE="xfs"</span><br><span class="line">/dev/sda2: UUID="Iylyc4-LvIV-TnAg-FZK6-xz2D-3gaY-hx17eb" TYPE="LVM2_member"</span><br><span class="line">/dev/sdb: UUID="104c6fd4-58e7-468f-a003-266d9a7fb9ba" TYPE="xfs"</span><br><span class="line">/dev/sdc: UUID="1a06d53d-0f76-4931-9add-fe3494510edc" TYPE="xfs"</span><br><span class="line">/dev/sdd: UUID="30a5527d-e7b6-4c13-b735-c9e086775d51" TYPE="xfs"</span><br><span class="line">/dev/sde1: UUID="58793fce-298c-417c-85dc-b0d913f8cd63" TYPE="xfs" PARTUUID="ca1a9c40-e020-4bf8-a17b-5716b3e1d453"</span><br><span class="line">/dev/sde2: UUID="1c7ef6a8-58a6-47c0-9d2d-ff086a3d81f7" TYPE="xfs" PARTUUID="c458e242-716d-42f7-9a2e-5f21c291987a"</span><br><span class="line">/dev/sde3: UUID="0581b33a-321d-42af-9b5f-272a775fccc1" TYPE="xfs" PARTUUID="651f90d2-9697-4474-b3d8-a0e980b125a4"</span><br><span class="line">/dev/mapper/centos-root: UUID="0fc63cb7-c2e6-46e8-8db9-2c31fdd20310" TYPE="xfs"</span><br><span class="line">/dev/mapper/centos-swap: UUID="a8605f7e-5049-4cd4-bd6d-805c98543f38" TYPE="swap"</span><br><span class="line">/dev/mapper/centos-home: UUID="7fc267af-b141-4140-afb9-388453097422" TYPE="xfs"</span><br><span class="line">[root@ceph-1 ceph]# ln -s /dev/disk/by-partuuid/ca1a9c40-e020-4bf8-a17b-5716b3e1d453  /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[root@ceph-1 ceph]# chown ceph:ceph -R /var/lib/ceph/osd/ceph-0</span><br><span class="line">[root@ceph-1 ceph]# chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[root@ceph-1 ceph]# ceph-osd --mkjournal -i 0</span><br><span class="line">2018-11-22 08:31:06.832760 7f97505afd80 -1 journal read_header error decoding journal header</span><br><span class="line">[root@ceph-1 ceph]# ceph-osd --mkjournal -i 0</span><br><span class="line">[root@ceph-1 ceph]# chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>注册osd.{id}，id为osd编号，默认从0开始</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sudo ceph auth add osd.&#123;id&#125; osd <span class="string">'allow *'</span> mon <span class="string">'allow profile osd'</span> -i /var/lib/ceph/osd/ceph-&#123;id&#125;/keyring</span></span><br><span class="line">[root@ceph-1 ceph]# ceph auth add osd.0 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">added key for osd.0</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>加入crush map</strong></p>
<p>这是ceph-1上新创建的第一个osd，CRUSH map中还没有ceph-1节点，因此首先要把ceph-1节点加入CRUSH map，同理，ceph-2/ceph-3节点也需要加入CRUSH map</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph osd crush add-bucket `hostname` host</span><br><span class="line">added bucket ceph-1 type host to crush map</span><br></pre></td></tr></table></figure>
<p>然后把三个节点移动到默认的root <code>default</code>下面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph osd crush move `hostname` root=default</span><br><span class="line">moved item id -2 name 'ceph-1' to location &#123;root=default&#125; in crush map</span><br></pre></td></tr></table></figure>
<p>添加osd.0到CRUSH map中的ceph-1节点下面，加入后，osd.0就能够接收数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">ceph osd crush add osd.&#123;id&#125; 0.4 root=sata rack=sata-rack01 host=sata-node5</span></span><br><span class="line"><span class="meta">#</span><span class="bash">0.4为此osd在CRUSH map中的权重值，它表示数据落在此osd上的比重，是一个相对值，一般按照1T磁盘比重值为1来计算，这里的osd数据盘1.7，所以值为1.7  </span></span><br><span class="line">[root@ceph-1 ceph]# ceph osd crush add osd.0 2.0 root=default host=`hostname`</span><br><span class="line">add item id 0 name 'osd.0' weight 2 at location &#123;host=ceph-1,root=default&#125; to crush map</span><br></pre></td></tr></table></figure>
<p>此时osd.0状态是<code>down</code>且<code>in</code>，<code>in</code>表示此osd位于CRUSH map，已经准备好接受数据，<code>down</code>表示osd进程运行异常，因为我们还没有启动osd.0进程</p>
</li>
<li>
<p><strong>启动ceph-osd进程</strong></p>
<p>需要向systemctl传递osd的<code>id</code>以启动指定的osd进程，如下，我们准备启动osd.0进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# cp /usr/lib/systemd/system/ceph-osd@.service /usr/lib/systemd/system/ceph-osd@0.service</span><br><span class="line">[root@ceph-1 ceph]# systemctl start ceph-osd@0</span><br><span class="line">[root@ceph-1 ceph]# systemctl enable ceph-osd@0</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /usr/lib/systemd/system/ceph-osd@0.service.</span><br></pre></td></tr></table></figure>
<p>上面就是添加osd.0的步骤，然后可以接着在其他<code>hostname</code>节点上添加osd.{1,2}，添加了这3个osd后，可以查看集群状态 ceph -s。</p>
</li>
<li>
<p><strong>验证osd进程是否成功启动</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ps -ef | grep ceph-osd</span><br><span class="line">ceph        2593       1  0 08:55 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph</span><br><span class="line">root        2697    1249  0 08:55 pts/0    00:00:00 grep --color=auto ceph-osd</span><br></pre></td></tr></table></figure>
<p>说明osd.0已经启动成功</p>
</li>
</ol>
<p>查看集群状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: ceph-1(active), standbys: admin, ceph-2, ceph-3</span><br><span class="line">    osd: 1 osds: 1 up, 1 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   107MiB used, 2.00TiB / 2.00TiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>osd已经有一个处于up跟in状态了。</p>
<p>按照上面的步骤重复操作，添加剩下的osd。</p>
<p>也可以执行如下脚本进行启动osd</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# vim start_osd.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">执行脚本时需要输入三个参数,顺序依次为：osd.num, osd数据存储分区，osd journal存储分区</span></span><br><span class="line"><span class="meta">#</span><span class="bash">例如：sudo sh start_osd.sh 0 /dev/sdc2 /dev/sdc1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">获取参数</span></span><br><span class="line">host_name=`hostname`</span><br><span class="line">id=$1</span><br><span class="line">data_disk=$2</span><br><span class="line">journal_disk=$3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建osd data</span></span><br><span class="line">ceph osd create $id</span><br><span class="line">sudo rm -rf /var/lib/ceph/osd/ceph-$id</span><br><span class="line">sudo mkdir /var/lib/ceph/osd/ceph-$id</span><br><span class="line"><span class="meta">#</span><span class="bash">sudo mkfs.xfs <span class="variable">$data_disk</span> -f</span></span><br><span class="line">sudo mount $data_disk /var/lib/ceph/osd/ceph-$id</span><br><span class="line">sudo ceph-osd -i $id --mkfs --mkkey</span><br><span class="line"><span class="meta">#</span><span class="bash">创建 journal</span></span><br><span class="line"><span class="meta">#</span><span class="bash">sudo mkfs.xfs <span class="variable">$journal_disk</span> -f</span></span><br><span class="line">uuid=`sudo blkid | grep $journal_disk | awk -F\" '&#123;print $6&#125;'`</span><br><span class="line">sudo rm -f /var/lib/ceph/osd/ceph-$id/journal </span><br><span class="line">sudo ln -s /dev/disk/by-partuuid/$uuid /var/lib/ceph/osd/ceph-$id/journal</span><br><span class="line">sudo chown ceph:ceph -R /var/lib/ceph/osd/ceph-$id</span><br><span class="line">sudo chown ceph:ceph /var/lib/ceph/osd/ceph-$id/journal</span><br><span class="line"><span class="meta">#</span><span class="bash">初始化新的journal</span></span><br><span class="line">sudo ceph-osd --mkjournal -i $id</span><br><span class="line">sudo chown ceph:ceph /var/lib/ceph/osd/ceph-$id/journal</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加osd到crushmap</span></span><br><span class="line">sudo ceph auth add osd.$id osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-$id/keyring</span><br><span class="line">sudo ceph osd crush add-bucket $host_name host</span><br><span class="line">sudo ceph osd crush move $host_name root=default</span><br><span class="line">sudo ceph osd crush add osd.$id 1.7 root=default host=$host_name</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动osd</span></span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-osd@.service /usr/lib/systemd/system/ceph-osd@$id.service</span><br><span class="line">sudo systemctl start ceph-osd@$id</span><br><span class="line">sudo systemctl enable ceph-osd@$id</span><br></pre></td></tr></table></figure>
<p>执行实例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 第一个参数：osd的id, 第二个参数： osd数据分区  第三个参数： osd的journal</span></span><br><span class="line">[root@ceph-1 ceph]# sh start_osd.sh 1 /dev/sdc /dev/sde2</span><br><span class="line">1</span><br><span class="line">2018-11-22 09:31:18.144721 7fc56f8b5d80 -1 auth: error reading file: /var/lib/ceph/osd/ceph-1/keyring: can't open /var/lib/ceph/osd/ceph-1/keyring: (2) No such file or directory</span><br><span class="line">2018-11-22 09:31:18.147361 7fc56f8b5d80 -1 created new key in keyring /var/lib/ceph/osd/ceph-1/keyring</span><br><span class="line">2018-11-22 09:31:18.184083 7fc56f8b5d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</span><br><span class="line">2018-11-22 09:31:18.208952 7fc56f8b5d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</span><br><span class="line">2018-11-22 09:31:18.209302 7fc56f8b5d80 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">2018-11-22 09:31:18.209321 7fc56f8b5d80 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">2018-11-22 09:31:18.209700 7fc56f8b5d80 -1 read_settings error reading settings: (2) No such file or directory</span><br><span class="line">2018-11-22 09:31:18.246674 7fc56f8b5d80 -1 created object store /var/lib/ceph/osd/ceph-1 for osd.1 fsid c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">2018-11-22 09:31:18.480406 7fce4eeb4d80 -1 journal read_header error decoding journal header</span><br><span class="line">added key for osd.1</span><br><span class="line">bucket 'ceph-1' already exists</span><br><span class="line">no need to move item id -2 name 'ceph-1' to location &#123;root=default&#125; in crush map</span><br><span class="line">add item id 1 name 'osd.1' weight 2 at location &#123;host=ceph-1,root=default&#125; to crush map</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /usr/lib/systemd/system/ceph-osd@1.service.</span><br></pre></td></tr></table></figure>
<p>按照上面的方法添加所有的osd之后，可以得到如下的集群：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-3 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: ceph-1(active), standbys: admin, ceph-2, ceph-3</span><br><span class="line">    osd: 9 osds: 9 up, 9 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   858MiB used, 16.0TiB / 16.0TiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<h2><span id="手动搭建mds-仅cephfs使用">手动搭建MDS (仅cephfs使用)</span></h2>
<ul>
<li>创建目录：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# mkdir /var/lib/ceph/mds/ceph-`hostname`</span><br><span class="line">[root@ceph-1 ceph]# chown ceph:ceph -R /var/lib/ceph/mds/ceph-`hostname`</span><br></pre></td></tr></table></figure>
<ul>
<li>在ceph.conf中添加如下信息：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[mds.ceph-1]</span><br><span class="line">host = ceph-1</span><br><span class="line">[mds.ceph-2]</span><br><span class="line">host = ceph-2</span><br><span class="line">[mds.ceph-3]</span><br><span class="line">host = ceph-3</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>重启ceph-mon</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# systemctl restart ceph-mon@`hostname`</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>启动mds</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# cp /usr/lib/systemd/system/ceph-mds@.service /usr/lib/systemd/system/ceph-mds@`hostname`.service</span><br><span class="line">[root@ceph-1 ceph]# systemctl start ceph-mds@`hostname`</span><br><span class="line">[root@ceph-1 ceph]# systemctl enable ceph-mds@`hostname`</span><br><span class="line">Created symlink from /etc/systemd/system/ceph-mds.target.wants/ceph-mds@ceph-1.service to /usr/lib/systemd/system/ceph-mds@ceph-1.service.</span><br></pre></td></tr></table></figure>
<ul>
<li>查看mds状态</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-1 ceph]# ceph mds stat</span><br><span class="line">, 1 up:standby</span><br></pre></td></tr></table></figure>
<p>在ceph-2,ceph-3上执行以上相同操作即可。</p>
<p>最终mds的状态为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-3 ceph]# ceph mds stat</span><br><span class="line">, 3 up:standby</span><br></pre></td></tr></table></figure>
<h1><span id="ceph集群搭建完成">Ceph集群搭建完成</span></h1>
<p>最终的ceph集群状态如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-3 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     c165f9d0-88df-48a7-8cc5-11da82f99c93</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: ceph-1(active), standbys: admin, ceph-2, ceph-3</span><br><span class="line">    osd: 9 osds: 9 up, 9 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   965MiB used, 18.0TiB / 18.0TiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>至此ceph集群搭建完成。</p>

            </div>
        
        </div>
        
    
    
        
<nav id="article-nav">
    
        <a href="4a310db5.html" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    第四篇：创建cephfs服务
                
            </div>
        </a>
    
    
        <a href="ad293d8.html" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">第二篇：Ceph集群环境准备</div>
        </a>
    
</nav>


    
</article>


    
    <section id="comments">
    
        <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<div class="gitalk">
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
            clientID: '6b346f4de8336f10e022',
            clientSecret: '706a95586b24195c087762bf6f2d49f0bf78735d',
            repo: 'cheneyoung.github.io',
            owner: 'cheneyoung',
            admin: ['cheneyoung'],
            id: "https://www.zuoyangblog.com/post/28a4bfb3.html".substr(19,50),      // Ensure uniqueness and length less than 50
            distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</div>
    
    </section>


</section>
            
            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2018 zuoyang<br>
               <!-- 首先引入统计用的js，然后编写相应的统计代码 -->
               <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
               访问量<span id="busuanzi_value_site_pv"></span>次  <i class="fa fa-user-md"></i>  访客数<span id="busuanzi_value_site_uv"></span>人
        </div>
    </div>
</footer>

        
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<div class="gitalk">
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
            clientID: '6b346f4de8336f10e022',
            clientSecret: '706a95586b24195c087762bf6f2d49f0bf78735d',
            repo: 'cheneyoung.github.io',
            owner: 'cheneyoung',
            admin: ['cheneyoung'],
            id: "https://www.zuoyangblog.com/post/28a4bfb3.html".substr(19,50),      // Ensure uniqueness and length less than 50
            distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</div>



    
        <script src="../libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="../libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="../libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="../libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="../libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="../libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="../libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="../libs/lightgallery/js/lg-share.min.js"></script>
        <script src="../libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="../libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    



<!-- Custom Scripts -->
<script src="../js/main.js"></script>

    </div>
</body>
</html>