<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    
    <title>手动部署ceph集群 | zuoyang&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="1、机器选择1.1 系统要求ceph 最新 LTS 版本 (luminous) 推荐 linux 内核版本 4.1.4 及以上, 最低版本要求 3.10.*。 1.2 服务器这里选择三台服务器来部署ceph集群，一台Mon+五台OSD     节点 服务 cluster network public network     192.168.226.20 osd.1,mon.node2 192.16">
<meta name="keywords" content="ceph">
<meta property="og:type" content="article">
<meta property="og:title" content="手动部署ceph集群">
<meta property="og:url" content="https://www.zuoyangblog.com/post/c3d7e91e.html">
<meta property="og:site_name" content="zuoyang&#39;s blog">
<meta property="og:description" content="1、机器选择1.1 系统要求ceph 最新 LTS 版本 (luminous) 推荐 linux 内核版本 4.1.4 及以上, 最低版本要求 3.10.*。 1.2 服务器这里选择三台服务器来部署ceph集群，一台Mon+五台OSD     节点 服务 cluster network public network     192.168.226.20 osd.1,mon.node2 192.16">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2018-11-15T14:56:49.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="手动部署ceph集群">
<meta name="twitter:description" content="1、机器选择1.1 系统要求ceph 最新 LTS 版本 (luminous) 推荐 linux 内核版本 4.1.4 及以上, 最低版本要求 3.10.*。 1.2 服务器这里选择三台服务器来部署ceph集群，一台Mon+五台OSD     节点 服务 cluster network public network     192.168.226.20 osd.1,mon.node2 192.16">
    

    

    
        <link rel="icon" href="../css/images/horse.jpg">
    

    <link rel="stylesheet" href="../libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="../libs/open-sans/styles.css">
    <link rel="stylesheet" href="../libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="../css/style.css">

    <script src="../libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="../libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="../libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129296219-1', 'auto');
ga('send', 'pageview');

</script>
    
    
    


</head>
</html>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="../index.html" id="logo">
                <i class="logo"></i>
                <span class="site-title">zuoyang&#39;s blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="../.">Home</a>
                
                    <a class="main-nav-link" href="../archives">Archives</a>
                
                    <a class="main-nav-link" href="../categories">Categories</a>
                
                    <a class="main-nav-link" href="../tags">Tags</a>
                
                    <a class="main-nav-link" href="../about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="../css/images/profile_pic.jpg">
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '../content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="../js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="../.">Home</a></td>
                
                    <td><a class="main-nav-link" href="../archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="../categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="../tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="../about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <script type='text/javascript'>
                $('body').append("<style type='text/css'>@media screen and (min-width: 1200px) { #main {margin: auto; width: 70%; display: block; float:none;}}</style>");
                </script>
                <script type='text/javascript'>
                document.querySelector('style').textContent += "@media screen and (max-width: 1199px) and (min-width: 800px) { #main {margin: auto; width: 70%; display: block; float:none;}}"
                </script>
            
            <section id="main"><article id="post-manual-deploy-ceph" class="article article-type-post" itemscope="" itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name" style="color: #009688;font-size:1.5em;">
            手动部署ceph集群
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="../categories/ceph/">ceph</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="../tags/ceph/">ceph</a>
    </div>

                        
                            <i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span>次
                        
                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <h2 id="1、机器选择"><a href="#1、机器选择" class="headerlink" title="1、机器选择"></a>1、机器选择</h2><h3 id="1-1-系统要求"><a href="#1-1-系统要求" class="headerlink" title="1.1 系统要求"></a>1.1 系统要求</h3><p>ceph 最新 LTS 版本 (luminous) 推荐 linux 内核版本 <code>4.1.4</code> 及以上, 最低版本要求 <code>3.10.*</code>。</p>
<h3 id="1-2-服务器"><a href="#1-2-服务器" class="headerlink" title="1.2 服务器"></a>1.2 服务器</h3><p>这里选择三台服务器来部署ceph集群，一台Mon+五台OSD</p>
<hr>
<table>
<thead>
<tr>
<th>节点</th>
<th>服务</th>
<th>cluster network</th>
<th>public network</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.226.20</td>
<td>osd.1,mon.node2</td>
<td>192.168.226.0/24</td>
<td>192.168.226.0/24</td>
</tr>
<tr>
<td>192.168.226.21</td>
<td>osd.4</td>
<td>192.168.226.0/24</td>
<td>192.168.226.0/24</td>
</tr>
<tr>
<td>192.168.226.22</td>
<td>osd.2, mon.node1</td>
<td>192.168.226.0/24</td>
<td>192.168.226.0/24</td>
</tr>
<tr>
<td>192.168.226.96</td>
<td>osd.3,mon.node3</td>
<td>192.168.226.0/24</td>
<td>192.168.226.0/24</td>
</tr>
<tr>
<td>192.168.226.106</td>
<td>osd.0</td>
<td>192.168.226.0/24</td>
<td>192.168.226.0/24</td>
</tr>
</tbody>
</table>
<p>每个节点只能使用1块磁盘部署osd。所以，集群共有5个<code>osd</code>进程，3个<code>monitor</code>进程。</p>
<p>cluster network 是处理osd间的数据复制，数据重平衡，osd进程心跳检测的网络，其不对外提供服务，只在各个osd节点间通信，本文使用eth1网卡作为cluster network，三个节点网卡eth1桥接到同一个网桥br1上</p>
<h2 id="2、环境配置"><a href="#2、环境配置" class="headerlink" title="2、环境配置"></a>2、环境配置</h2><p>配置每个节点的host文件，在 <code>/etc/hosts</code>文件中添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.226.20 ceph-1</span><br><span class="line">192.168.226.22 ceph-2</span><br><span class="line">192.168.226.96 ceph-3</span><br></pre></td></tr></table></figure>
<h3 id="2-2-ceph节点安装"><a href="#2-2-ceph节点安装" class="headerlink" title="2.2 ceph节点安装"></a>2.2 ceph节点安装</h3><p>你的管理节点必须能够通过 SSH 无密码地访问各 Ceph 节点。如果 <code>ceph-deploy</code> 以某个普通用户登录，那么这个用户必须有无密码使用 <code>sudo</code> 的权限。</p>
<h4 id="2-2-1-安装-NTP"><a href="#2-2-1-安装-NTP" class="headerlink" title="2.2.1 安装 NTP"></a>2.2.1 安装 NTP</h4><p>我们建议在所有 Ceph 节点上安装 NTP 服务（特别是 Ceph Monitor 节点），以免因时钟漂移导致故障，详情见<a href="http://docs.ceph.org.cn/rados/configuration/mon-config-ref#clock" target="_blank" rel="noopener">时钟</a>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install ntp ntpdate ntp-doc</span><br></pre></td></tr></table></figure>
<p>确保在各 Ceph 节点上启动了 NTP 服务，并且要使用同一个 NTP 服务器，详情见 <a href="http://www.ntp.org/" target="_blank" rel="noopener">NTP</a> 。</p>
<h4 id="2-2-2-安装-SSH-服务器"><a href="#2-2-2-安装-SSH-服务器" class="headerlink" title="2.2.2 安装 SSH 服务器"></a>2.2.2 安装 SSH 服务器</h4><p>在<strong>所有 Ceph</strong> 节点上执行如下步骤：</p>
<ol>
<li><p>在各 Ceph 节点安装 SSH 服务器（如果还没有）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install openssh-server</span><br></pre></td></tr></table></figure>
</li>
<li><p>确保<strong>所有</strong> Ceph 节点上的 SSH 服务器都在运行。</p>
</li>
</ol>
<h4 id="2-2-3-安装ceph"><a href="#2-2-3-安装ceph" class="headerlink" title="2.2.3 安装ceph"></a>2.2.3 安装ceph</h4><p>由于蚂蚁内部物理机不能访问外网，使用以下步骤安装ceph。</p>
<p>在<strong>所有Ceph</strong>节点上执行如下步骤：</p>
<p>下载ceph所有的依赖rpm，并解压缩</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo wget http://qianli-lzh.oss-cn-hangzhou-zmf.aliyuncs.com/bill_inference_public%2Fceph.tar</span><br><span class="line">sudo tar -xvf bill_inference_public%2Fceph.tar</span><br></pre></td></tr></table></figure>
<p>手动安装所有的rpm</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -ivh --force --nodeps ceph/*.rpm</span><br></pre></td></tr></table></figure>
<p>验证ceph是否正确安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph -v</span><br><span class="line">ceph version 12.2.8 (ae699615bac534ea496ee965ac6192cb7e0e07c0) luminous (stable)</span><br></pre></td></tr></table></figure>
<h4 id="2-2-4-关闭防火墙"><a href="#2-2-4-关闭防火墙" class="headerlink" title="2.2.4 关闭防火墙"></a>2.2.4 关闭防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config</span><br><span class="line">sudo setenforce 0</span><br><span class="line">sudo systemctl stop firewalld </span><br><span class="line">sudo systemctl disable firewalld</span><br></pre></td></tr></table></figure>
<h2 id="3、集群搭建"><a href="#3、集群搭建" class="headerlink" title="3、集群搭建"></a>3、集群搭建</h2><h3 id="3-1-搭建Mon集群-使用admin账户"><a href="#3-1-搭建Mon集群-使用admin账户" class="headerlink" title="3.1 搭建Mon集群 (使用admin账户)"></a>3.1 搭建Mon集群 (使用admin账户)</h3><p><strong>创建配置文件</strong></p>
<p>在<strong>每台节点机器</strong>上创建配置文件<code>/etc/ceph/ceph.conf</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">fsid = 932XXXXX-fba7-XXXX-9526-a858c613f468</span><br><span class="line">mon initial members = e15p13447.ew9</span><br><span class="line">mon host = 192.168.226.20,192.168.226.22,192.168.226.96</span><br><span class="line">rbd default features = 1</span><br><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br><span class="line">public network = 192.168.226.0/24</span><br><span class="line">cluster network = 192.168.226.0/24</span><br><span class="line">osd journal size = 1024</span><br><span class="line">osd pool default size = 2</span><br><span class="line">osd pool default min size = 1</span><br><span class="line">osd pool default pg num = 128</span><br><span class="line">osd pool default pgp num = 128</span><br><span class="line">osd crush chooseleaf type = 1</span><br><span class="line">mon_max_pg_per_osd = 200</span><br><span class="line"></span><br><span class="line">[mds.ceph-1]</span><br><span class="line">host = ceph-1</span><br><span class="line">[mds.ceph-2]</span><br><span class="line">host = ceph-2</span><br><span class="line">[mds.ceph-3]</span><br><span class="line">host = ceph-3</span><br><span class="line"></span><br><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true</span><br></pre></td></tr></table></figure>
<p>其中 <code>fsid</code> 是为集群分配的一个 uuid, 初始化 mon 节点其实只需要这一个配置就够了。<br><code>mon host</code> 配置 ceph 命令行工具访问操作 ceph 集群时查找 mon 节点入口。<br>ceph 集群可包含多个 mon 节点实现高可用容灾, 避免单点故障。<br><code>rbd default features = 1</code> 配置 rbd 客户端创建磁盘时禁用一些需要高版本内核才能支持的特性。</p>
<h4 id="3-1-2-主mon节点-（192-168-226-20）"><a href="#3-1-2-主mon节点-（192-168-226-20）" class="headerlink" title="3.1.2 主mon节点 （192.168.226.20）"></a>3.1.2 主mon节点 （192.168.226.20）</h4><p>1、为此集群创建密钥环、并生成Monitor密钥 (3台机器一样)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'</span><br></pre></td></tr></table></figure>
<p>2、生成管理员密钥环，生成 <code>client.admin</code> 用户并加入密钥环 (3台机器一样)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'</span><br></pre></td></tr></table></figure>
<p>3、把 <code>client.admin</code> 密钥加入 <code>ceph.mon.keyring</code>  (3台机器一样)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure>
<p>4、用规划好的主机名、对应 IP 地址、和 FSID 生成一个Monitor Map，并保存为 <code>/tmp/monmap</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">host_name=`hostname`</span><br><span class="line">sudo monmaptool --create --add $host_name 192.168.226.20  --fsid 932XXXXX-fba7-XXXX-9526-a858c613f468 /tmp/monmap --clobber</span><br></pre></td></tr></table></figure>
<p>5、在Monitor主机上分别创建数据目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">host_name=`hostname`</span><br><span class="line"><span class="meta">#</span><span class="bash">在admin账户下</span></span><br><span class="line">sudo mkdir /var/lib/ceph/mon/ceph-$host_name/</span><br></pre></td></tr></table></figure>
<p>6、用Monitor Map和密钥环组装守护进程所需的初始数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph-mon --mkfs -i $host_name --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</span><br></pre></td></tr></table></figure>
<p>7、建一个空文件 <code>done</code> ，表示监视器已创建、可以启动了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo touch /var/lib/ceph/mon/ceph-$host_name/done</span><br></pre></td></tr></table></figure>
<p>8、启动Monitor</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">sudo ceph-mon -f --cluster ceph --id <span class="variable">$host_name</span> &amp;</span></span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-mon@.service /usr/lib/systemd/system/ceph-mon@$host_name.service</span><br><span class="line">sudo systemctl start ceph-mon@$host_name</span><br><span class="line">sudo systemctl enable ceph-mon@$host_name</span><br></pre></td></tr></table></figure>
<p>9、确认下集群在运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph -s</span><br></pre></td></tr></table></figure>
<p>事例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cluster:</span><br><span class="line">  id:     932XXXXX-fba7-XXXX-9526-a858c613f468</span><br><span class="line">  health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">services:</span><br><span class="line">  mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">  mgr: no daemons active</span><br><span class="line">  osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">data:</span><br><span class="line">  pools:   0 pools, 0 pgs</span><br><span class="line">  objects: 0 objects, 0B</span><br><span class="line">  usage:   0B used, 0B / 0B avail</span><br><span class="line">  pgs:</span><br></pre></td></tr></table></figure>
<h4 id="3-1-2-从mon节点-192-168-226-22-amp-192-168-226-96"><a href="#3-1-2-从mon节点-192-168-226-22-amp-192-168-226-96" class="headerlink" title="3.1.2 从mon节点 (192.168.226.22 &amp; 192.168.226.96)"></a>3.1.2 从mon节点 (192.168.226.22 &amp; 192.168.226.96)</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">host_name=`hostname`</span><br><span class="line">sudo ceph mon getmap -o /tmp/monmap</span><br><span class="line">sudo rm -rf /var/lib/ceph/mon/ceph-$host_name</span><br><span class="line">sudo ceph-mon -i $host_name --mkfs --monmap /tmp/monmap</span><br><span class="line">sudo chown -R ceph:ceph /var/lib/ceph/mon/ceph-$host_name/</span><br><span class="line"><span class="meta">#</span><span class="bash">nohup ceph-mon -f --cluster ceph --id <span class="variable">$host_name</span> --setuser ceph --setgroup ceph &amp;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">ceph-mon -f --cluster ceph --id <span class="variable">$host_name</span> &amp;</span></span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-mon@.service /usr/lib/systemd/system/ceph-mon@$host_name.service</span><br><span class="line">sudo systemctl start ceph-mon@$host_name</span><br><span class="line">sudo systemctl enable ceph-mon@$host_name</span><br></pre></td></tr></table></figure>
<h3 id="3-2-创建ceph-mgr"><a href="#3-2-创建ceph-mgr" class="headerlink" title="3.2 创建ceph-mgr"></a>3.2 创建ceph-mgr</h3><h4 id="3-2-1-创建用户-openstack-用于-MGR-监控"><a href="#3-2-1-创建用户-openstack-用于-MGR-监控" class="headerlink" title="3.2.1 创建用户 openstack 用于 MGR 监控"></a>3.2.1 创建用户 openstack 用于 MGR 监控</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create mgr.openstack mon 'allow *' osd 'allow *' mds 'allow *'</span><br><span class="line">输出：</span><br><span class="line">[mgr.openstack]</span><br><span class="line">        key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxugvXkLfgauLA==</span><br></pre></td></tr></table></figure>
<p>需要将之前创建的用户密码存放至对应位置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/lib/ceph/mgr/ceph-openstack</span><br><span class="line">ceph auth get mgr.openstack -o  /var/lib/ceph/mgr/ceph-openstack/keyring</span><br><span class="line">exported keyring for mgr.openstack</span><br></pre></td></tr></table></figure>
<h4 id="3-2-2-启动mgr"><a href="#3-2-2-启动mgr" class="headerlink" title="3.2.2 启动mgr"></a>3.2.2 启动mgr</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-mgr -i openstack</span><br></pre></td></tr></table></figure>
<p>监控状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">ceph -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    id:     932e88a6-fba7-45a9-9526-a858c613f468</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3</span><br><span class="line">    mgr: openstack(active)</span><br><span class="line">    mds: cephfs-1/1/1 up  &#123;0=2=up:active&#125;, 2 up:standby</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   2 pools, 256 pgs</span><br><span class="line">    objects: 21 objects, 3.04KiB</span><br><span class="line">    usage:   3.32GiB used, 1.17TiB / 1.17TiB avail</span><br><span class="line">    pgs:     256 active+clean</span><br></pre></td></tr></table></figure>
<p>当 mgr 服务被激活之后, service 中 mgr 会显示 mgr-$name(active)<br>data 部分信息将变得可用</p>
<h3 id="3-3-手动搭建osd集群-三台机器上做相同的操作，注意osd-id的变化"><a href="#3-3-手动搭建osd集群-三台机器上做相同的操作，注意osd-id的变化" class="headerlink" title="3.3 手动搭建osd集群(三台机器上做相同的操作，注意osd_id的变化)"></a>3.3 手动搭建osd集群(三台机器上做相同的操作，注意osd_id的变化)</h3><p>添加一个新osd，<code>id</code>可以省略，ceph会自动使用最小可用整数，第一个osd从0开始</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">ceph osd create &#123;id&#125;</span></span><br><span class="line">ceph osd create</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<h4 id="3-3-1-初始化osd目录"><a href="#3-3-1-初始化osd目录" class="headerlink" title="3.3.1 初始化osd目录"></a>3.3.1 初始化osd目录</h4><p>创建osd.0目录，目录名格式<code>{cluster-name}-{id}</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;id&#125;</span></span><br><span class="line">sudo mkdir /var/lib/ceph/osd/ceph-0</span><br></pre></td></tr></table></figure>
<p>挂载osd.0的数据盘/dev/sdb2</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.xfs /dev/sdb2</span><br><span class="line">sudo mount /dev/sdb2 /var/lib/ceph/osd/ceph-0</span><br></pre></td></tr></table></figure>
<p>初始化osd数据目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sudo ceph-osd -i &#123;id&#125; --mkfs --mkkey</span></span><br><span class="line">sudo ceph-osd -i 0 --mkfs --mkkey</span><br><span class="line"><span class="meta">#</span><span class="bash">--mkkey要求osd数据目录为空</span></span><br><span class="line"><span class="meta">#</span><span class="bash">这会创建osd.0的keyring /var/lib/ceph/osd/ceph-0/keyring</span></span><br></pre></td></tr></table></figure>
<p>初始化后，默认使用普通文件/var/lib/ceph/osd/ceph-3/journal作为osd.0的journal分区，普通文件作为journal分区性能不高，若只是测试环境，可以跳过更改journal分区这一步骤</p>
<h4 id="3-3-2-创建journal"><a href="#3-3-2-创建journal" class="headerlink" title="3.3.2 创建journal"></a>3.3.2 创建journal</h4><p>生成journal分区，一般选ssd盘作为journal分区，这里使用ssd的/dev/sdb1分区作为journal</p>
<p>使用fdisk工分出磁盘/dev/sdb1,</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">清除磁盘所有分区(重新添加时需要)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">sgdisk --zap-all --clear --mbrtogpt /dev/sdb</span></span><br><span class="line"><span class="meta">#</span><span class="bash">生成分区/dev/sdb1的uuid</span></span><br><span class="line"><span class="meta">#</span><span class="bash">uuidgen</span></span><br><span class="line"><span class="meta">#</span><span class="bash">b3897364-8807-48eb-9905-e2c8400d0cd4</span></span><br><span class="line"><span class="meta">#</span><span class="bash">创建分区</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1:0:+100G 表示创建第一个分区，100G大小</span></span><br><span class="line"><span class="meta">#</span><span class="bash">sudo sgdisk --new=1:0:+100G --change-name=1:<span class="string">'ceph journal'</span> --partition-guid=1:b3897364-8807-48eb-9905-e2c8400d0cd4 --typecode=1:b3897364-8807-48eb-9905-e2c8400d0cd4 --mbrtogpt -- /dev/vdf</span></span><br><span class="line"><span class="meta">#</span><span class="bash">格式化</span></span><br><span class="line">sudo mkfs.xfs /dev/sdb1</span><br><span class="line">sudo rm -f /var/lib/ceph/osd/ceph-4/journal </span><br><span class="line"><span class="meta">#</span><span class="bash">查看分区对应的partuuid， 找出/dev/sdb1对应的partuuid</span></span><br><span class="line">sudo blkid</span><br><span class="line">sudo ln -s /dev/disk/by-partuuid/b3897364-8807-48eb-9905-e2c8400d0cd4 /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line"></span><br><span class="line">sudo chown ceph:ceph -R /var/lib/ceph/osd/ceph-0</span><br><span class="line">sudo chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line"><span class="meta">#</span><span class="bash">初始化新的journal</span></span><br><span class="line">sudo ceph-osd --mkjournal -i 0</span><br><span class="line">sudo chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span><br></pre></td></tr></table></figure>
<h4 id="3-3-3-注册osd-id-，id为osd编号，默认从0开始"><a href="#3-3-3-注册osd-id-，id为osd编号，默认从0开始" class="headerlink" title="3.3.3 注册osd.{id}，id为osd编号，默认从0开始"></a>3.3.3 注册osd.{id}，id为osd编号，默认从0开始</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sudo ceph auth add osd.&#123;id&#125; osd <span class="string">'allow *'</span> mon <span class="string">'allow profile osd'</span> -i /var/lib/ceph/osd/ceph-&#123;id&#125;/keyring</span></span><br><span class="line">sudo ceph auth add osd.0 osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line"><span class="meta">#</span><span class="bash">ceph auth list 中出现osd.0</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-4-加入crush-map"><a href="#3-3-4-加入crush-map" class="headerlink" title="3.3.4 加入crush map"></a>3.3.4 加入crush map</h4><p>这是m1上新创建的第一个osd，CRUSH map中还没有m1节点，因此首先要把m1节点加入CRUSH map，同理，m2/m3节点也需要加入CRUSH map</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">ceph osd crush add-bucket &#123;hostname&#125; host</span></span><br><span class="line">sudo ceph osd crush add-bucket `hostname` host</span><br></pre></td></tr></table></figure>
<p>然后把三个节点移动到默认的root <code>default</code>下面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ceph osd crush move `hostname` root=default</span><br></pre></td></tr></table></figure>
<p>添加osd.0到CRUSH map中的m1节点下面，加入后，osd.0就能够接收数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">ceph osd crush add osd.&#123;id&#125; 0.4 root=sata rack=sata-rack01 host=sata-node5</span></span><br><span class="line">sudo ceph osd crush add osd.4 1.7 root=default host=`hostname`</span><br><span class="line"><span class="meta">#</span><span class="bash">0.4为此osd在CRUSH map中的权重值，它表示数据落在此osd上的比重，是一个相对值，一般按照1T磁盘比重值为1来计算，这里的osd数据盘1.7，所以值为1.7</span></span><br></pre></td></tr></table></figure>
<p>此时osd.0状态是<code>down</code>且<code>in</code>，<code>in</code>表示此osd位于CRUSH map，已经准备好接受数据，<code>down</code>表示osd进程运行异常，因为我们还没有启动osd.0进程</p>
<h4 id="3-3-5-启动ceph-osd进程"><a href="#3-3-5-启动ceph-osd进程" class="headerlink" title="3.3.5 启动ceph-osd进程"></a>3.3.5 启动ceph-osd进程</h4><p>需要向systemctl传递osd的<code>id</code>以启动指定的osd进程，如下，我们准备启动osd.0进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">systemctl start ceph-osd@&#123;id&#125;  id表示osd编号，从数字0开始</span></span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-osd@.service /usr/lib/systemd/system/ceph-osd@0.service</span><br><span class="line">sudo systemctl start ceph-osd@0</span><br><span class="line">sudo systemctl enable ceph-osd@0</span><br><span class="line"><span class="meta">#</span><span class="bash">sudo ceph-osd -i 0</span></span><br></pre></td></tr></table></figure>
<p>上面就是添加osd.0的步骤，然后可以接着在其他<code>hostname</code>节点上添加osd.{1,2}，添加了这3个osd后，可以查看集群状态 ceph -s。</p>
<h3 id="3-4-搭建MDS"><a href="#3-4-搭建MDS" class="headerlink" title="3.4 搭建MDS"></a>3.4 搭建MDS</h3><p>创建目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /var/lib/ceph/mds/ceph-`hostname`</span><br><span class="line">sudo chown ceph:ceph -R /var/lib/ceph/mds/ceph-`hostname`</span><br></pre></td></tr></table></figure>
<p>在ceph.conf中添加如下信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[mds.&#123;id&#125;]</span><br><span class="line">host = &#123;id&#125;</span><br><span class="line">例如：</span><br><span class="line">[mds.0]</span><br><span class="line">host = 0</span><br></pre></td></tr></table></figure>
<p>启动mds</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">ceph-mds --cluster &#123;cluster-name&#125; -i &#123;id&#125; -m &#123;mon-hostname&#125;:&#123;mon-port&#125; [-f]</span></span><br><span class="line">sudo cp /usr/lib/systemd/system/ceph-mds@.service /usr/lib/systemd/system/ceph-mds@`hostname`.service </span><br><span class="line">sudo systemctl start ceph-mds@`hostname`</span><br><span class="line">sudo systemctl enable ceph-mds@`hostname`</span><br><span class="line"><span class="meta">#</span><span class="bash">ceph-mds --cluster ceph -i 0 -m e15p13447.ew9:6789</span></span><br></pre></td></tr></table></figure>
<p>查看mds状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph mds stat</span><br><span class="line">cephfs-1/1/1 up  &#123;0=1=up:active&#125;, 2 up:standby</span><br></pre></td></tr></table></figure>
<p>至此ceph集群搭建完成。</p>

            </div>
        
        </div>
        
    
    
        
<nav id="article-nav">
    
        <a href="f055ffc7.html" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    最后一个单词的长度
                
            </div>
        </a>
    
    
</nav>


    
</article>


    
    <section id="comments">
    
        <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<div class="gitalk">
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
            clientID: '6b346f4de8336f10e022',
            clientSecret: '706a95586b24195c087762bf6f2d49f0bf78735d',
            repo: 'cheneyoung.github.io',
            owner: 'cheneyoung',
            admin: ['cheneyoung'],
            id: "https://www.zuoyangblog.com/post/c3d7e91e.html".substr(19,50),      // Ensure uniqueness and length less than 50
            distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</div>
    
    </section>


</section>
            
            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2018 zuoyang<br>
               <!-- 首先引入统计用的js，然后编写相应的统计代码 -->
               <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
               访问量<span id="busuanzi_value_site_pv"></span>次  <i class="fa fa-user-md"></i>  访客数<span id="busuanzi_value_site_uv"></span>人
        </div>
    </div>
</footer>

        
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<div class="gitalk">
    <div id="gitalk-container"></div>
    <script type="text/javascript">
        const gitalk = new Gitalk({
            clientID: '6b346f4de8336f10e022',
            clientSecret: '706a95586b24195c087762bf6f2d49f0bf78735d',
            repo: 'cheneyoung.github.io',
            owner: 'cheneyoung',
            admin: ['cheneyoung'],
            id: "https://www.zuoyangblog.com/post/c3d7e91e.html".substr(19,50),      // Ensure uniqueness and length less than 50
            distractionFreeMode: false  // Facebook-like distraction free mode
        })

        gitalk.render('gitalk-container')
    </script>
</div>



    
        <script src="../libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="../libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="../libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="../libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="../libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="../libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="../libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="../libs/lightgallery/js/lg-share.min.js"></script>
        <script src="../libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="../libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    



<!-- Custom Scripts -->
<script src="../js/main.js"></script>

    </div>
</body>
</html>